#!/usr/bin/env python3
"""
GitHub íŒŒì´ì¬ ì½”ë“œ í¬ë¡¤ëŸ¬

GitHubì—ì„œ íŒŒì´ì¬ ì½”ë“œë¥¼ ê²€ìƒ‰í•˜ê³  ë‹¤ìš´ë¡œë“œí•˜ëŠ” ëª¨ë“ˆì…ë‹ˆë‹¤.
í•™ìŠµìš©ìœ¼ë¡œ ì í•©í•œ ì½”ë“œë¥¼ í•„í„°ë§í•˜ëŠ” ê¸°ëŠ¥ì„ í¬í•¨í•©ë‹ˆë‹¤.
"""

import os
import base64
import time
import json
from datetime import datetime
from github import Github, RateLimitExceededException
import requests
from dotenv import load_dotenv
import os
from code_filter import CodeQualityFilter

load_dotenv()  # â¬…ï¸ .env íŒŒì¼ì„ ì½ì–´ì„œ os.environ ì— ë“±ë¡
token = os.getenv("GITHUB_TOKEN") 
print(f"[DEBUG] Loaded token: {token}")

class GitHubPythonCrawler:
    """GitHubì—ì„œ íŒŒì´ì¬ ì½”ë“œë¥¼ í¬ë¡¤ë§í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, token=None, output_dir="collected_code"):
        """
        í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
        
        Args:
            token (str, optional): GitHub API í† í°. ì—†ìœ¼ë©´ ì œí•œëœ API ì‚¬ìš©
            output_dir (str, optional): ìˆ˜ì§‘ëœ ì½”ë“œë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬
        """
        self.github = Github(token) if token else Github()
        self.output_dir = output_dir
        self.metadata_file = os.path.join(output_dir, "metadata.json")
        self.quality_filter = CodeQualityFilter(metadata_file=self.metadata_file)
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(output_dir, exist_ok=True)
        
        # ë©”íƒ€ë°ì´í„° íŒŒì¼ ì´ˆê¸°í™”
        if not os.path.exists(self.metadata_file):
            with open(self.metadata_file, 'w', encoding='utf-8') as f:
                json.dump([], f)
                
    
    def search_repositories(self, query="language:python", sort="stars", 
                           order="desc", max_results=10):
        """
        GitHubì—ì„œ íŒŒì´ì¬ ì €ì¥ì†Œ ê²€ìƒ‰
        
        Args:
            query (str): ê²€ìƒ‰ ì¿¼ë¦¬
            sort (str): ì •ë ¬ ê¸°ì¤€ (stars, forks, updated)
            order (str): ì •ë ¬ ìˆœì„œ (desc, asc)
            max_results (int): ìµœëŒ€ ê²°ê³¼ ìˆ˜
            
        Returns:
            list: ê²€ìƒ‰ëœ ì €ì¥ì†Œ ëª©ë¡
        """
        print(f"GitHubì—ì„œ '{query}' ê²€ìƒ‰ ì¤‘...")
        try:
            repositories = self.github.search_repositories(
                query=query, sort=sort, order=order
            )
            
            results = []
            count = 0
            
            for repo in repositories:
                if count >= max_results:
                    break
                
                results.append({
                    'name': repo.name,
                    'full_name': repo.full_name,
                    'url': repo.html_url,
                    'description': repo.description,
                    'stars': repo.stargazers_count,
                    'forks': repo.forks_count,
                    'created_at': repo.created_at.isoformat(),
                    'updated_at': repo.updated_at.isoformat(),
                    'license': repo.license.name if repo.license else None
                })
                count += 1
                
            print(f"{len(results)}ê°œì˜ ì €ì¥ì†Œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
            return results
            
        except RateLimitExceededException:
            reset_time = self.github.get_rate_limit().core.reset
            current_time = datetime.utcnow()
            wait_time = (reset_time - current_time).total_seconds()
            print(f"API ì‚¬ìš©ëŸ‰ ì œí•œ ì´ˆê³¼. {wait_time:.0f}ì´ˆ í›„ì— ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
            return []
    
    def get_python_files(self, repo_name, max_files=50):
        """
        ì €ì¥ì†Œì—ì„œ íŒŒì´ì¬ íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        
        Args:
            repo_name (str): ì €ì¥ì†Œ ì´ë¦„ (ì˜ˆ: 'username/repo')
            max_files (int): ìµœëŒ€ íŒŒì¼ ìˆ˜
            
        Returns:
            list: íŒŒì´ì¬ íŒŒì¼ ì •ë³´ ëª©ë¡
        """
        print(f"{repo_name} ì €ì¥ì†Œì—ì„œ íŒŒì´ì¬ íŒŒì¼ ê²€ìƒ‰ ì¤‘...")
        try:
            repo = self.github.get_repo(repo_name)
            contents = repo.get_contents("")
            python_files = []
            
            while contents and len(python_files) < max_files:
                file_content = contents.pop(0)
                
                if file_content.type == "dir":
                    contents.extend(repo.get_contents(file_content.path))
                elif file_content.name.endswith(".py"):
                    python_files.append({
                        'name': file_content.name,
                        'path': file_content.path,
                        'url': file_content.html_url,
                        'sha': file_content.sha,
                        'size': file_content.size
                    })
            
            print(f"{len(python_files)}ê°œì˜ íŒŒì´ì¬ íŒŒì¼ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
            return python_files
            
        except Exception as e:
            print(f"íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° ì˜¤ë¥˜: {str(e)}")
            return []
    
    def download_file(self, repo_name, file_path):
        """
        GitHubì—ì„œ íŒŒì¼ ë‚´ìš© ë‹¤ìš´ë¡œë“œ
        
        Args:
            repo_name (str): ì €ì¥ì†Œ ì´ë¦„
            file_path (str): íŒŒì¼ ê²½ë¡œ
            
        Returns:
            str: íŒŒì¼ ë‚´ìš©
        """
        try:
            repo = self.github.get_repo(repo_name)
            file_content = repo.get_contents(file_path)
            
            if isinstance(file_content, list):
                return None  # ë””ë ‰í† ë¦¬ì¸ ê²½ìš°
            
            # base64ë¡œ ì¸ì½”ë”©ëœ ë‚´ìš© ë””ì½”ë”©
            content = base64.b64decode(file_content.content).decode('utf-8')
            return content
            
        except Exception as e:
            print(f"íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì˜¤ë¥˜ ({repo_name}/{file_path}): {str(e)}")
            return None
    
    def save_file(self, repo_name, file_path, content):
        """
        íŒŒì¼ ë‚´ìš©ì„ ë¡œì»¬ì— ì €ì¥
        
        Args:
            repo_name (str): ì €ì¥ì†Œ ì´ë¦„
            file_path (str): íŒŒì¼ ê²½ë¡œ
            content (str): íŒŒì¼ ë‚´ìš©
            
        Returns:
            str: ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
        """
        if content is None:
            return None
            
        # ì €ì¥ ê²½ë¡œ ìƒì„±
        repo_dir = os.path.join(self.output_dir, repo_name.replace('/', '_'))
        os.makedirs(repo_dir, exist_ok=True)
        
        # íŒŒì¼ ì´ë¦„ ì¶”ì¶œ ë° ì €ì¥
        file_name = os.path.basename(file_path)
        save_path = os.path.join(repo_dir, file_name)
        
        try:
            with open(save_path, 'w', encoding='utf-8') as f:
                f.write(content)
            return save_path
        except Exception as e:
            print(f"íŒŒì¼ ì €ì¥ ì˜¤ë¥˜: {str(e)}")
            return None
    
    def update_metadata(self, repo_info, file_info, local_path, quality_score=None):
        """
        ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
        
        Args:
            repo_info (dict): ì €ì¥ì†Œ ì •ë³´
            file_info (dict): íŒŒì¼ ì •ë³´
            local_path (str): ë¡œì»¬ ì €ì¥ ê²½ë¡œ
            quality_score (float, optional): ì½”ë“œ í’ˆì§ˆ ì ìˆ˜
        """
        try:
            with open(self.metadata_file, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
                
            # ìƒˆ ë©”íƒ€ë°ì´í„° í•­ëª© ì¶”ê°€
            metadata.append({
                'repo_name': repo_info['name'],
                'repo_full_name': repo_info['full_name'],
                'repo_url': repo_info['url'],
                'repo_stars': repo_info['stars'],
                'repo_license': repo_info['license'],
                'file_name': file_info['name'],
                'file_path': file_info['path'],
                'file_url': file_info['url'],
                'local_path': local_path,
                'quality_score': quality_score,
                'downloaded_at': datetime.now().isoformat()
            })
            
            with open(self.metadata_file, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, indent=2)
                
        except Exception as e:
            print(f"ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {str(e)}")
    
    def crawl(self, query="language:python", max_repos=5, max_files_per_repo=10):
        """
        GitHubì—ì„œ íŒŒì´ì¬ ì½”ë“œ í¬ë¡¤ë§ ì‹¤í–‰ + í’ˆì§ˆ í‰ê°€
        
        Args:
            query (str): ê²€ìƒ‰ ì¿¼ë¦¬
            max_repos (int): ìµœëŒ€ ì €ì¥ì†Œ ìˆ˜
            max_files_per_repo (int): ì €ì¥ì†Œë‹¹ ìµœëŒ€ íŒŒì¼ ìˆ˜
            
        Returns:
            list: ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ ì •ë³´ ëª©ë¡
        """
        # í’ˆì§ˆ í‰ê°€ ë„êµ¬ ì´ˆê¸°í™”
        quality_filter = CodeQualityFilter(metadata_file=self.metadata_file)

        # ì €ì¥ì†Œ ê²€ìƒ‰
        repositories = self.search_repositories(query=query, max_results=max_repos)
        downloaded_files = []

        for repo in repositories:
            print(f"\nì €ì¥ì†Œ ì²˜ë¦¬ ì¤‘: {repo['full_name']}")
            python_files = self.get_python_files(repo['full_name'], max_files=max_files_per_repo)

            for file_info in python_files:
                time.sleep(0.5)  # API ì œí•œ ë°©ì§€

                content = self.download_file(repo['full_name'], file_info['path'])
                if content:
                    local_path = self.save_file(repo['full_name'], file_info['path'], content)
                    if local_path:
                        # âœ… í’ˆì§ˆ ë¶„ì„ ìˆ˜í–‰
                        quality_score = quality_filter.evaluate_code_quality(local_path)
                        code_lines = quality_filter.count_code_lines(local_path)
                        complexity_info = quality_filter.check_code_complexity(local_path)
                        is_suitable, reason = quality_filter.is_suitable_for_learning(local_path, repo)

                        # ë©”íƒ€ë°ì´í„° ì €ì¥
                        self.update_metadata(
                            repo_info=repo,
                            file_info=file_info,
                            local_path=local_path,
                            quality_score=quality_score
                        )

                        # ğŸ‘‰ ë©”íƒ€ë°ì´í„°ì— ë¶€ê°€ ì •ë³´ ì§ì ‘ ì¶”ê°€
                        with open(self.metadata_file, 'r', encoding='utf-8') as f:
                            metadata = json.load(f)
                        if metadata:
                            metadata[-1]['code_lines'] = code_lines
                            metadata[-1]['complexity'] = complexity_info
                            metadata[-1]['is_suitable'] = is_suitable
                            metadata[-1]['unsuitable_reason'] = None if is_suitable else reason
                            with open(self.metadata_file, 'w', encoding='utf-8') as f:
                                json.dump(metadata, f, indent=2)

                        downloaded_files.append({
                            'repo': repo['full_name'],
                            'file': file_info['path'],
                            'local_path': local_path
                        })
                        print(f"íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë° í’ˆì§ˆ ë¶„ì„ ì™„ë£Œ: {file_info['path']}")

        print(f"\nì´ {len(downloaded_files)}ê°œì˜ íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        return downloaded_files

    def crawl_repository(self, username, repo_name, max_files_per_repo=10):
        """
        íŠ¹ì • GitHub ì €ì¥ì†Œì—ì„œ íŒŒì´ì¬ íŒŒì¼ì„ í¬ë¡¤ë§í•˜ê³  ì €ì¥

        Args:
            username (str): ì‚¬ìš©ì ì´ë¦„
            repo_name (str): ì €ì¥ì†Œ ì´ë¦„
            max_files_per_repo (int): ìµœëŒ€ íŒŒì¼ ìˆ˜
        """
        full_name = f"{username}/{repo_name}"
        print(f"ğŸ” ì €ì¥ì†Œ {full_name} ì—ì„œ ì½”ë“œ í¬ë¡¤ë§ ì¤‘...")

        repo_info = {
            'name': repo_name,
            'full_name': full_name,
            'url': f"https://github.com/{full_name}",
            'description': '',
            'stars': 0,
            'forks': 0,
            'created_at': '',
            'updated_at': '',
            'license': None
        }

        python_files = self.get_python_files(full_name, max_files=max_files_per_repo)
        downloaded_files = []

        for file_info in python_files:
            content = self.download_file(full_name, file_info['path'])
            if content:
                local_path = self.save_file(full_name, file_info['path'], content)
                if local_path:
                    self.update_metadata(repo_info, file_info, local_path)
                    downloaded_files.append({
                        'repo': full_name,
                        'file': file_info['path'],
                        'local_path': local_path
                    })
                    print(f"ğŸ“¥ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {file_info['path']}")

        print(f"âœ… ì €ì¥ì†Œ í¬ë¡¤ë§ ì™„ë£Œ: {len(downloaded_files)}ê°œì˜ íŒŒì¼ ë‹¤ìš´ë¡œë“œë¨")
        return downloaded_files

    



# í…ŒìŠ¤íŠ¸ ì½”ë“œ
if __name__ == "__main__":
    # í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    crawler = GitHubPythonCrawler(output_dir="collected_code")
    
    # í¬ë¡¤ë§ ì‹¤í–‰ (í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì ì€ ìˆ˜ì˜ ì €ì¥ì†Œì™€ íŒŒì¼ë§Œ ê°€ì ¸ì˜´)
    crawler.crawl(query="language:python stars:>1000", max_repos=1, max_files_per_repo=1)
